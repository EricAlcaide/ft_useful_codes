{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Strategies to evolve Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import pchip\n",
    "from random import random, randint\n",
    "import tensorflow as tf\n",
    "from operator import add\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an individual here // n_layers + space for accuracy\n",
    "def individual(n_layers, minimum, maximum, alpha_max, alpha_min):\n",
    "\treturn [ randint(minimum, maximum) for n in range(n_layers)]+[randint(alpha_min, alpha_max), 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the population here\n",
    "def population(n_nets, n_layers, minimum, maximum, alpha_max, alpha_min):\n",
    "\treturn [ individual(n_layers, minimum, maximum, alpha_max, alpha_min) for n in range(n_nets) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure the fitness of an entire population. Lower is better.\n",
    "def evaluate(population):\n",
    "\ttotal = reduce(add, ([n[-1] for n in population]), 0)\n",
    "\treturn total / float(len(population))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolve individuals and create the next generation. Select the 20% best. Not the best approach\n",
    "def evolve(population, minimum, maximum, n_nets, n_layers, retain, random_aditional, mutation):\n",
    "    tupled = [ neural(net) for net in population ]\n",
    "    tupled = [ net for net in sorted(tupled, key=lambda x: x[-1], reverse=True)]\n",
    "    retain_length = int(retain*n_nets)\n",
    "    \n",
    "    # Print the best Net of the generation\n",
    "    print(\"Best of that Generation: \",tupled[0])\n",
    "    # Continue\n",
    "    \n",
    "    parents = tupled[:retain_length]\n",
    "    # Select other individuals randomly to maintain genetic diversity. Could be avoided with a better approach.\n",
    "    for net in tupled[retain_length:]:\n",
    "        if random_aditional > random():\n",
    "            parents.append(net)\n",
    "    # Mutate some individuals to maintain genetic diversity\n",
    "    for net in parents:\n",
    "        for i in range(n_layers):\n",
    "            if mutation > random():\n",
    "                net[i] = randint(minimum, maximum)\n",
    "    # Crossover of parents to generate children\n",
    "    parents_length = len(parents)\n",
    "    children_maxlength = n_nets - parents_length\n",
    "    children = []\n",
    "    while len(children) < children_maxlength:\n",
    "        male = randint(0, parents_length-1)\n",
    "        female = randint(0, parents_length-1)\n",
    "        if male != female:\n",
    "            male = parents[male]\n",
    "            female = parents[female]\n",
    "            cross_point = randint(0, n_layers+2)\n",
    "            child = male[:cross_point]+female[cross_point:] \t\t\t# Combine male and female\n",
    "            children.append(child)\n",
    "    parents.extend(children)\t\t\t\t\t\t\t\t\t\t\t# Extend parents list by appending children list\n",
    "    return parents \t\t\t\t\t\t\t\t\t\t\t\t# Return the next Generation of individuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the accuracy for a given neural net\n",
    "def neural(net):\n",
    "    \n",
    "    batch_size = 100\n",
    "    K = net[0]\n",
    "    L = net[1]\n",
    "    alpha = net[2]/100000\n",
    "\n",
    "    W1 = tf.Variable(tf.truncated_normal([28*28, K], stddev = 0.1))\n",
    "    B1 = tf.Variable(tf.zeros([K]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([K, L], stddev = 0.1))\n",
    "    B2 = tf.Variable(tf.zeros([L]))\n",
    "    W3 = tf.Variable(tf.truncated_normal([L, 10], stddev = 0.1))\n",
    "    B3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, 28 * 28]) \t# One layer because 28*28 gray-scaled images, the None will become the batch size\n",
    "    X = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "    # Defining the model - changing relu by my_function\n",
    "    Y1 = tf.nn.relu(tf.matmul(X, W1) + B1)\n",
    "    Y2 = tf.nn.relu(tf.matmul(Y1, W2) + B2)\n",
    "    Y3 = tf.nn.softmax(tf.matmul(Y2, W3) + B3)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Defining the placeholder for correct answers\n",
    "    Y_ = tf.placeholder(tf.float32, [None, 10]) \t\t# \"One-hot\" encoded vector (00001000000)\n",
    "\n",
    "    # SUCCESS metrics\n",
    "    # Loss function to determine how bad is the model\n",
    "    cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y3)) \t# If Y is 1, log(Y) = 0, if Y is 0, log(Y) = -infinite\n",
    "    # % of correct answers found in batch\n",
    "    is_correct = tf.equal(tf.argmax(Y3, 1), tf.argmax(Y_, 1))\t# \"One-hot\" decoding here\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "    # Training step\n",
    "    optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "    # RUN the fuckin' code here\n",
    "    sess = tf.Session() \t\t\t\t\t\t\t\t# Code in tf is not computed until RUN\n",
    "    sess.run(init)\n",
    "    # Training loop\n",
    "    for i in range(125):\n",
    "        # Load batch of images and correct answers\n",
    "        batch_X, batch_Y = mnist.train.next_batch(batch_size)\t# Train on mini_batches of 100 images\n",
    "        train_data = {X: batch_X, Y_: batch_Y}\n",
    "        # Train\n",
    "        sess.run(train_step, feed_dict = train_data)\n",
    "        # {X: batch_X, Y_: batch_Y}\n",
    "        \n",
    "    # Succes on training data\n",
    "    a_tr, c_tr = sess.run([accuracy, cross_entropy], feed_dict = train_data)\n",
    "    # Success on test data?\n",
    "    test_data = {X: mnist.test.images, Y_: mnist.test.labels}\n",
    "    a_test, c_test = sess.run([accuracy, cross_entropy], feed_dict = test_data)\n",
    "    print(\"Net (1st:\"+str(K)+\", 2nd:\"+str(L)+\", Alpha:\"+str(alpha)+\"): acc_train:\", a_tr, \"|| acc_test:\", a_test)\n",
    "    \n",
    "    sess.close() # Closing the session\n",
    "    \n",
    "    return [K, L, alpha*1000, a_test*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (1st:56, 2nd:81): acc_train: 0.89 || acc_test: 0.9118\n",
      "[56, 81, 0.00085, 91.180002689361572]\n",
      "Net (1st:2, 2nd:45): acc_train: 0.66 || acc_test: 0.5794\n",
      "[2, 45, 0.00114, 57.940000295639038]\n",
      "Net (1st:50, 2nd:60): acc_train: 0.96 || acc_test: 0.9282\n",
      "[50, 60, 0.0017, 92.820000648498535]\n",
      "Net (1st:22, 2nd:86): acc_train: 0.9 || acc_test: 0.9035\n",
      "[22, 86, 0.00111, 90.35000205039978]\n",
      "Net (1st:9, 2nd:93): acc_train: 0.94 || acc_test: 0.8979\n",
      "[9, 93, 0.0018, 89.789998531341553]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(neural([randint(1,100), randint(1,100), randint(1,300), 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare parameters\n",
    "N_MAX_NEURONS = 100\n",
    "N_MIN_NEURONS = 1\n",
    "ALPHA_MAX = 300\n",
    "ALPHA_MIN = 1\n",
    "N_LAYERS = 2\n",
    "N_NETS = 10\n",
    "N_ITER = 5\n",
    "RETAIN = 0.2\n",
    "MUTATION = RANDOM_ADD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Fitness of each generation. Lower is better\n",
    "def plot_graph(eval_history):\n",
    "    x = [g for g in range(len(eval_history))]\n",
    "    # Data to be interpolated.\n",
    "    y = eval_history\n",
    "    # Define plots.\n",
    "    plt.plot(x, y, 'r.')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Graph visualization\")\n",
    "    plt.xlabel(\"Generations\")\n",
    "    plt.ylabel(\"Average Accuracy after 10o learning episodes\")\n",
    "    # Plot it all\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (1st:51, 2nd:1, Alpha:0.00105): acc_train: 0.24 || acc_test: 0.2077\n",
      "Net (1st:95, 2nd:81, Alpha:0.00105): acc_train: 0.92 || acc_test: 0.8731\n",
      "Net (1st:32, 2nd:3, Alpha:0.00225): acc_train: 0.42 || acc_test: 0.4271\n",
      "Net (1st:55, 2nd:61, Alpha:0.003): acc_train: 0.94 || acc_test: 0.8695\n",
      "Net (1st:18, 2nd:82, Alpha:0.00085): acc_train: 0.71 || acc_test: 0.766\n",
      "Net (1st:36, 2nd:72, Alpha:0.00223): acc_train: 0.91 || acc_test: 0.8769\n",
      "Net (1st:90, 2nd:18, Alpha:0.00198): acc_train: 0.84 || acc_test: 0.7779\n",
      "Net (1st:28, 2nd:51, Alpha:0.00041): acc_train: 0.53 || acc_test: 0.5143\n",
      "Net (1st:35, 2nd:31, Alpha:0.00106): acc_train: 0.82 || acc_test: 0.7131\n",
      "Net (1st:34, 2nd:24, Alpha:0.00018): acc_train: 0.2 || acc_test: 0.2643\n",
      "Best of that Generation:  [36, 72, 2.2300000000000004, 87.690001726150513]\n",
      "------ GEN 1 ------\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000003e-05): acc_train: 0.08 || acc_test: 0.1548\n",
      "Net (1st:95, 2nd:81, Alpha:1.0500000000000001e-05): acc_train: 0.15 || acc_test: 0.1651\n",
      "Net (1st:95, 2nd:81, Alpha:1.0500000000000001e-05): acc_train: 0.1 || acc_test: 0.142\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000003e-05): acc_train: 0.06 || acc_test: 0.1073\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000003e-05): acc_train: 0.21 || acc_test: 0.1502\n",
      "Net (1st:95, 2nd:81, Alpha:2.2300000000000003e-05): acc_train: 0.1 || acc_test: 0.111\n",
      "Net (1st:95, 2nd:81, Alpha:1.0500000000000001e-05): acc_train: 0.07 || acc_test: 0.0866\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000003e-05): acc_train: 0.08 || acc_test: 0.1055\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000003e-05): acc_train: 0.09 || acc_test: 0.1349\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-05): acc_train: 0.07 || acc_test: 0.1326\n",
      "Best of that Generation:  [95, 81, 0.0105, 16.509999334812164]\n",
      "------ GEN 2 ------\n",
      "Net (1st:95, 2nd:81, Alpha:1.05e-07): acc_train: 0.09 || acc_test: 0.0939\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000005e-07): acc_train: 0.05 || acc_test: 0.0635\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000005e-07): acc_train: 0.06 || acc_test: 0.1188\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000005e-07): acc_train: 0.05 || acc_test: 0.0661\n",
      "Net (1st:95, 2nd:81, Alpha:2.2300000000000005e-07): acc_train: 0.13 || acc_test: 0.1404\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000005e-07): acc_train: 0.06 || acc_test: 0.1133\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000005e-07): acc_train: 0.07 || acc_test: 0.0843\n",
      "Net (1st:95, 2nd:81, Alpha:2.2300000000000005e-07): acc_train: 0.11 || acc_test: 0.1046\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000005e-07): acc_train: 0.12 || acc_test: 0.1107\n",
      "Net (1st:36, 2nd:72, Alpha:1.05e-07): acc_train: 0.1 || acc_test: 0.0538\n",
      "Best of that Generation:  [95, 81, 0.00022300000000000005, 14.040000736713409]\n",
      "------ GEN 3 ------\n",
      "Net (1st:95, 2nd:81, Alpha:2.2300000000000004e-09): acc_train: 0.11 || acc_test: 0.118\n",
      "Net (1st:36, 2nd:72, Alpha:2.2300000000000004e-09): acc_train: 0.13 || acc_test: 0.1177\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000004e-09): acc_train: 0.13 || acc_test: 0.12\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000004e-09): acc_train: 0.09 || acc_test: 0.0858\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000004e-09): acc_train: 0.12 || acc_test: 0.1272\n",
      "Net (1st:95, 2nd:81, Alpha:2.2300000000000004e-09): acc_train: 0.07 || acc_test: 0.055\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000004e-09): acc_train: 0.08 || acc_test: 0.0965\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000004e-09): acc_train: 0.11 || acc_test: 0.1076\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000004e-09): acc_train: 0.03 || acc_test: 0.0726\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000004e-09): acc_train: 0.11 || acc_test: 0.0933\n",
      "Best of that Generation:  [95, 72, 2.2300000000000002e-06, 12.720000743865967]\n",
      "------ GEN 4 ------\n",
      "Net (1st:62, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.07 || acc_test: 0.1033\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.1 || acc_test: 0.0952\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.07 || acc_test: 0.0786\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.17 || acc_test: 0.1177\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.07 || acc_test: 0.0896\n",
      "Net (1st:62, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.08 || acc_test: 0.0918\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.13 || acc_test: 0.1344\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.19 || acc_test: 0.1514\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.13 || acc_test: 0.0918\n",
      "Net (1st:95, 2nd:72, Alpha:2.2300000000000003e-11): acc_train: 0.14 || acc_test: 0.1252\n",
      "Best of that Generation:  [95, 72, 2.2300000000000004e-08, 15.139999985694885]\n",
      "------ GEN 5 ------\n",
      "------------------------------\n",
      "------------------------------\n",
      "[0.0, 87.576000690460205, 15.89199960231781, 11.019000113010406, 11.968000158667564, 11.159999743103981]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0V0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1960fabc080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pop = population(N_NETS, N_LAYERS, N_MIN_NEURONS, N_MAX_NEURONS, ALPHA_MAX, ALPHA_MIN)\n",
    "evaluation = evaluate(pop)\n",
    "eval_history = [evaluation] # Record w/ the average acc of all generations\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    pop = evolve(pop, N_MIN_NEURONS, N_MAX_NEURONS, N_NETS, N_LAYERS, RETAIN, RANDOM_ADD, MUTATION)\n",
    "    print(\"------ GEN \"+str(i+1)+\" ------\")\n",
    "    eval_history.append(evaluate(pop))\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"------------------------------\")\n",
    "print(eval_history)\n",
    "plot_graph(eval_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
