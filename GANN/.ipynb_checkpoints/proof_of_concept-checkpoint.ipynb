{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Strategies to evolve Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import pchip\n",
    "from random import random, randint\n",
    "import tensorflow as tf\n",
    "from operator import add\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an individual here // n_layers + space for accuracy\n",
    "def individual(n_layers, minimum, maximum, alpha_max, alpha_min):\n",
    "\treturn [ randint(minimum, maximum) for n in range(n_layers)]+[randint(alpha_min, alpha_max), 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the population here\n",
    "def population(n_nets, n_layers, minimum, maximum, alpha_max, alpha_min):\n",
    "\treturn [ individual(n_layers, minimum, maximum, alpha_max, alpha_min) for n in range(n_nets) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure the fitness of an entire population. Lower is better.\n",
    "def evaluate(population):\n",
    "\ttotal = reduce(add, ([n[-1] for n in population]), 0)\n",
    "\treturn total / float(len(population))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolve individuals and create the next generation. Select the 20% best. Not the best approach\n",
    "def evolve(population, minimum, maximum, n_nets, n_layers, retain, random_aditional, mutation):\n",
    "    tupled = [ reversed(neural(net)) for net in population ]\n",
    "    tupled = [ reversed(net) for net in sorted(tupled, reverse=True) ]\n",
    "    retain_length = int(retain*n_nets)\n",
    "    \n",
    "    # Print the best Net of the generation\n",
    "    print(\"Best of that Generation: \",tupled[0])\n",
    "    # Continue\n",
    "    \n",
    "    parents = tupled[:retain_length]\n",
    "    # Select other individuals randomly to maintain genetic diversity. Could be avoided with a better approach.\n",
    "    for net in tupled[retain_length:]:\n",
    "        if random_aditional > random():\n",
    "            parents.append(net)\n",
    "    # Mutate some individuals to maintain genetic diversity\n",
    "    for net in parents:\n",
    "        for i in range(n_layers+1):\n",
    "            if mutation > random():\n",
    "                net[i] = randint(minimum, maximum)\n",
    "    # Crossover of parents to generate children\n",
    "    parents_length = len(parents)\n",
    "    children_maxlength = n_nets - parents_length\n",
    "    children = []\n",
    "    while len(children) < children_maxlength:\n",
    "        male = randint(0, parents_length-1)\n",
    "        female = randint(0, parents_length-1)\n",
    "        if male != female:\n",
    "            male = parents[male]\n",
    "            female = parents[female]\n",
    "            cross_point = randint(0, n_layers+2)\n",
    "            child = male[:cross_point]+female[cross_point:] \t\t\t# Combine male and female\n",
    "            children.append(child)\n",
    "    parents.extend(children)\t\t\t\t\t\t\t\t\t\t\t# Extend parents list by appending children list\n",
    "    return parents \t\t\t\t\t\t\t\t\t\t\t\t# Return the next Generation of individuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the accuracy for a given neural net\n",
    "def neural(net):\n",
    "    \n",
    "    batch_size = 100\n",
    "    K = net[0]\n",
    "    L = net[1]\n",
    "    alpha = net[2]/100000\n",
    "\n",
    "    W1 = tf.Variable(tf.truncated_normal([28*28, K], stddev = 0.1))\n",
    "    B1 = tf.Variable(tf.zeros([K]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([K, L], stddev = 0.1))\n",
    "    B2 = tf.Variable(tf.zeros([L]))\n",
    "    W3 = tf.Variable(tf.truncated_normal([L, 10], stddev = 0.1))\n",
    "    B3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, 28 * 28]) \t# One layer because 28*28 gray-scaled images, the None will become the batch size\n",
    "    X = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "    # Defining the model - changing relu by my_function\n",
    "    Y1 = tf.nn.relu(tf.matmul(X, W1) + B1)\n",
    "    Y2 = tf.nn.relu(tf.matmul(Y1, W2) + B2)\n",
    "    Y3 = tf.nn.softmax(tf.matmul(Y2, W3) + B3)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Defining the placeholder for correct answers\n",
    "    Y_ = tf.placeholder(tf.float32, [None, 10]) \t\t# \"One-hot\" encoded vector (00001000000)\n",
    "\n",
    "    # SUCCESS metrics\n",
    "    # Loss function to determine how bad is the model\n",
    "    cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y3)) \t# If Y is 1, log(Y) = 0, if Y is 0, log(Y) = -infinite\n",
    "    # % of correct answers found in batch\n",
    "    is_correct = tf.equal(tf.argmax(Y3, 1), tf.argmax(Y_, 1))\t# \"One-hot\" decoding here\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "    # Training step\n",
    "    optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "    # RUN the fuckin' code here\n",
    "    sess = tf.Session() \t\t\t\t\t\t\t\t# Code in tf is not computed until RUN\n",
    "    sess.run(init)\n",
    "    # Training loop\n",
    "    for i in range(100):\n",
    "        # Load batch of images and correct answers\n",
    "        batch_X, batch_Y = mnist.train.next_batch(batch_size)\t# Train on mini_batches of 100 images\n",
    "        train_data = {X: batch_X, Y_: batch_Y}\n",
    "        # Train\n",
    "        sess.run(train_step, feed_dict = train_data)\n",
    "        # {X: batch_X, Y_: batch_Y}\n",
    "        \n",
    "    # Succes on training data\n",
    "    a_tr, c_tr = sess.run([accuracy, cross_entropy], feed_dict = train_data)\n",
    "    # Success on test data?\n",
    "    test_data = {X: mnist.test.images, Y_: mnist.test.labels}\n",
    "    a_test, c_test = sess.run([accuracy, cross_entropy], feed_dict = test_data)\n",
    "    print(\"Net (1st:\"+str(K)+\", 2nd:\"+str(L)+\", Alpha:\"+str(alpha)+\"): acc_train:\", a_tr, \"|| acc_test:\", a_test)\n",
    "    \n",
    "    sess.close() # Closing the session\n",
    "    \n",
    "    return [K, L, alpha, a_test*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (1st:56, 2nd:81): acc_train: 0.89 || acc_test: 0.9118\n",
      "[56, 81, 0.00085, 91.180002689361572]\n",
      "Net (1st:2, 2nd:45): acc_train: 0.66 || acc_test: 0.5794\n",
      "[2, 45, 0.00114, 57.940000295639038]\n",
      "Net (1st:50, 2nd:60): acc_train: 0.96 || acc_test: 0.9282\n",
      "[50, 60, 0.0017, 92.820000648498535]\n",
      "Net (1st:22, 2nd:86): acc_train: 0.9 || acc_test: 0.9035\n",
      "[22, 86, 0.00111, 90.35000205039978]\n",
      "Net (1st:9, 2nd:93): acc_train: 0.94 || acc_test: 0.8979\n",
      "[9, 93, 0.0018, 89.789998531341553]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(neural([randint(1,100), randint(1,100), randint(1,300), 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare parameters\n",
    "N_MAX_NEURONS = 100\n",
    "N_MIN_NEURONS = 1\n",
    "ALPHA_MAX = 300\n",
    "ALPHA_MIN = 1\n",
    "N_LAYERS = 2\n",
    "N_NETS = 10\n",
    "N_ITER = 5\n",
    "RETAIN = 0.2\n",
    "MUTATION = RANDOM_ADD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Fitness of each generation. Lower is better\n",
    "def plot_graph(eval_history):\n",
    "    x = [g for g in range(len(eval_history))]\n",
    "    # Data to be interpolated.\n",
    "    y = eval_history\n",
    "    # Define plots.\n",
    "    plt.plot(x, y, 'r.')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Graph visualization\")\n",
    "    plt.xlabel(\"Generations\")\n",
    "    plt.ylabel(\"Average Accuracy after 10o learning episodes\")\n",
    "    # Plot it all\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (1st:97, 2nd:19, Alpha:0.00177): acc_train: 0.9 || acc_test: 0.8411\n",
      "Net (1st:54, 2nd:39, Alpha:0.0024): acc_train: 0.91 || acc_test: 0.8755\n",
      "Net (1st:57, 2nd:54, Alpha:0.00116): acc_train: 0.79 || acc_test: 0.8049\n",
      "Net (1st:65, 2nd:69, Alpha:0.00237): acc_train: 0.88 || acc_test: 0.8466\n",
      "Net (1st:49, 2nd:38, Alpha:0.00015): acc_train: 0.31 || acc_test: 0.2532\n",
      "Net (1st:65, 2nd:43, Alpha:0.00186): acc_train: 0.9 || acc_test: 0.8479\n",
      "Net (1st:94, 2nd:66, Alpha:0.0016): acc_train: 0.91 || acc_test: 0.8762\n",
      "Net (1st:57, 2nd:36, Alpha:0.00279): acc_train: 0.93 || acc_test: 0.8729\n",
      "Net (1st:1, 2nd:18, Alpha:0.00081): acc_train: 0.1 || acc_test: 0.121\n",
      "Net (1st:18, 2nd:94, Alpha:0.00037): acc_train: 0.55 || acc_test: 0.4455\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: list_reverseiterator() < list_reverseiterator()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-49ce053b64d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_ITER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_MIN_NEURONS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_MAX_NEURONS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_NETS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_LAYERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRETAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRANDOM_ADD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMUTATION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"------ GEN \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" ------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0meval_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-6498f676c200>\u001b[0m in \u001b[0;36mevolve\u001b[1;34m(population, minimum, maximum, n_nets, n_layers, retain, random_aditional, mutation)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminimum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaximum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_nets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_aditional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtupled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneural\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtupled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtupled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mretain_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn_nets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unorderable types: list_reverseiterator() < list_reverseiterator()"
     ]
    }
   ],
   "source": [
    "pop = population(N_NETS, N_LAYERS, N_MIN_NEURONS, N_MAX_NEURONS, ALPHA_MAX, ALPHA_MIN)\n",
    "evaluation = evaluate(pop)\n",
    "eval_history = [evaluation] # Record w/ the average acc of all generations\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    pop = evolve(pop, N_MIN_NEURONS, N_MAX_NEURONS, N_NETS, N_LAYERS, RETAIN, RANDOM_ADD, MUTATION)\n",
    "    print(\"------ GEN \"+str(i+1)+\" ------\")\n",
    "    eval_history.append(evaluate(pop))\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"------------------------------\")\n",
    "print(eval_history)\n",
    "plot_graph(eval_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
