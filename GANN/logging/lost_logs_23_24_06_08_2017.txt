line 375 +

INFO 2017-08-06 21:52:52,032 - Generation Average: 45.8605
INFO 2017-08-06 21:52:52,032 - --------------------------------------------------------
INFO 2017-08-06 21:52:52,032 - Parents: [{'activation': 'sigmoid', 'dropout': 0.05, 'nb_neurons': 1024, 'optimizer': 'adamax', 'nb_layers': 4}, {'activation': 'sigmoid', 'dropout': 0.1, 'nb_neurons': 1024, 'optimizer': 'adamax', 'nb_layers': 3}, {'activation': 'elu', 'dropout': 0.05, 'nb_neurons': 256, 'optimizer': 'adam', 'nb_layers': 3}, {'activation': 'selu', 'dropout': 0.15, 'nb_neurons': 512, 'optimizer': 'adamax', 'nb_layers': 1}, {'activation': 'selu', 'dropout': 0.1, 'nb_neurons': 1024, 'optimizer': 'adamax', 'nb_layers': 1}]
INFO 2017-08-06 21:52:52,032 - *************************************************
INFO 2017-08-06 21:52:52,032 - ------ GEN 3 ------
INFO 2017-08-06 21:52:52,032 - Saving the accuracy result of the training
INFO 2017-08-06 21:52:52,032 - Number of layers: 4
INFO 2017-08-06 21:52:52,032 - Number of neurons: 1024
INFO 2017-08-06 21:52:52,033 - Activation: sigmoid
INFO 2017-08-06 21:52:52,033 - Optimizer: adamax
INFO 2017-08-06 21:52:52,033 - Dropout: 0.05
INFO 2017-08-06 22:49:22,933 - Acc @ Training: 0%
INFO 2017-08-06 22:49:22,933 - Acc @ Testing: 54.43%
INFO 2017-08-06 22:49:22,933 - --------------------------------------------------------
INFO 2017-08-06 22:49:22,933 - Saving the accuracy result of the training
INFO 2017-08-06 22:49:22,933 - Number of layers: 3
INFO 2017-08-06 22:49:22,933 - Number of neurons: 1024
INFO 2017-08-06 22:49:22,933 - Activation: sigmoid
INFO 2017-08-06 22:49:22,933 - Optimizer: adamax
INFO 2017-08-06 22:49:22,933 - Dropout: 0.1



+375 
INFO 2017-08-07 14:49:03,690 - Parents: [{'nb_neurons': 1024, 'nb_layers': 4, 'optimizer': 'adamax', 'dropout': 0.05, 'activation': 'sigmoid'}, {'nb_neurons': 1024, 'nb_layers': 3, 'optimizer': 'adamax', 'dropout': 0.1, 'activation': 'sigmoid'}, {'nb_neurons': 256, 'nb_layers': 3, 'optimizer': 'adam', 'dropout': 0.05, 'activation': 'elu'}, {'nb_neurons': 512, 'nb_layers': 1, 'optimizer': 'adamax', 'dropout': 0.15, 'activation': 'selu'}]
INFO 2017-08-07 14:49:03,690 - *************************************************
INFO 2017-08-07 14:49:03,690 - ------ GEN 3 ------
INFO 2017-08-07 14:49:03,690 - Saving the accuracy result of the training
INFO 2017-08-07 14:49:03,690 - Number of layers: 4
INFO 2017-08-07 14:49:03,690 - Number of neurons: 1024
INFO 2017-08-07 14:49:03,690 - Activation: sigmoid
INFO 2017-08-07 14:49:03,690 - Optimizer: adamax
INFO 2017-08-07 14:49:03,690 - Dropout: 0.05
INFO 2017-08-07 15:51:09,321 - Acc @ Training: 0%
INFO 2017-08-07 15:51:09,327 - Acc @ Testing: 51.31%
INFO 2017-08-07 15:51:09,327 - --------------------------------------------------------
INFO 2017-08-07 15:51:09,328 - Saving the accuracy result of the training
INFO 2017-08-07 15:51:09,328 - Number of layers: 3
INFO 2017-08-07 15:51:09,328 - Number of neurons: 1024
INFO 2017-08-07 15:51:09,328 - Activation: sigmoid
INFO 2017-08-07 15:51:09,328 - Optimizer: adamax
INFO 2017-08-07 15:51:09,328 - Dropout: 0.1
INFO 2017-08-07 16:44:57,477 - Acc @ Training: 0%
INFO 2017-08-07 16:44:57,479 - Acc @ Testing: 53.72%
INFO 2017-08-07 16:44:57,479 - --------------------------------------------------------
INFO 2017-08-07 16:44:57,479 - Saving the accuracy result of the training
INFO 2017-08-07 16:44:57,479 - Number of layers: 3
INFO 2017-08-07 16:44:57,479 - Number of neurons: 256
INFO 2017-08-07 16:44:57,479 - Activation: sigmoid
INFO 2017-08-07 16:44:57,479 - Optimizer: adam
INFO 2017-08-07 16:44:57,479 - Dropout: 0.05
INFO 2017-08-07 16:53:36,111 - Acc @ Training: 0%
INFO 2017-08-07 16:53:36,111 - Acc @ Testing: 48.19%
INFO 2017-08-07 16:53:36,112 - --------------------------------------------------------
INFO 2017-08-07 16:53:36,112 - Saving the accuracy result of the training
INFO 2017-08-07 16:53:36,112 - Number of layers: 1
INFO 2017-08-07 16:53:36,112 - Number of neurons: 512
INFO 2017-08-07 16:53:36,112 - Activation: selu
INFO 2017-08-07 16:53:36,112 - Optimizer: adamax
INFO 2017-08-07 16:53:36,112 - Dropout: 0.2
INFO 2017-08-07 17:08:54,567 - Acc @ Training: 0%
INFO 2017-08-07 17:08:54,567 - Acc @ Testing: 52.75%
INFO 2017-08-07 17:08:54,567 - --------------------------------------------------------
INFO 2017-08-07 17:08:54,567 - Saving the accuracy result of the training
INFO 2017-08-07 17:08:54,567 - Number of layers: 1
INFO 2017-08-07 17:08:54,568 - Number of neurons: 1024
INFO 2017-08-07 17:08:54,568 - Activation: selu
INFO 2017-08-07 17:08:54,568 - Optimizer: adamax
INFO 2017-08-07 17:08:54,568 - Dropout: 0.2
INFO 2017-08-07 17:32:47,688 - Acc @ Training: 0%
INFO 2017-08-07 17:32:47,688 - Acc @ Testing: 50.69%
INFO 2017-08-07 17:32:47,688 - --------------------------------------------------------
INFO 2017-08-07 17:32:47,688 - Saving the accuracy result of the training
INFO 2017-08-07 17:32:47,688 - Number of layers: 1
INFO 2017-08-07 17:32:47,688 - Number of neurons: 1024
INFO 2017-08-07 17:32:47,688 - Activation: selu
INFO 2017-08-07 17:32:47,688 - Optimizer: adamax
INFO 2017-08-07 17:32:47,688 - Dropout: 0.1
INFO 2017-08-07 17:53:00,347 - Acc @ Training: 0%
INFO 2017-08-07 17:53:00,347 - Acc @ Testing: 49.27%
INFO 2017-08-07 17:53:00,347 - --------------------------------------------------------
INFO 2017-08-07 17:53:00,347 - Saving the accuracy result of the training
INFO 2017-08-07 17:53:00,347 - Number of layers: 3
INFO 2017-08-07 17:53:00,347 - Number of neurons: 512
INFO 2017-08-07 17:53:00,347 - Activation: selu
INFO 2017-08-07 17:53:00,347 - Optimizer: adam
INFO 2017-08-07 17:53:00,347 - Dropout: 0.2
INFO 2017-08-07 18:33:12,705 - Acc @ Training: 0%
INFO 2017-08-07 18:33:12,707 - Acc @ Testing: 53.67%
INFO 2017-08-07 18:33:12,707 - --------------------------------------------------------
INFO 2017-08-07 18:33:12,707 - Saving the accuracy result of the training
INFO 2017-08-07 18:33:12,707 - Number of layers: 1
INFO 2017-08-07 18:33:12,707 - Number of neurons: 1024
INFO 2017-08-07 18:33:12,707 - Activation: sigmoid
INFO 2017-08-07 18:33:12,707 - Optimizer: adamax
INFO 2017-08-07 18:33:12,707 - Dropout: 0.05
INFO 2017-08-07 19:01:56,485 - Acc @ Training: 0%
INFO 2017-08-07 19:01:56,485 - Acc @ Testing: 52.38%
INFO 2017-08-07 19:01:56,485 - --------------------------------------------------------
INFO 2017-08-07 19:01:56,485 - Saving the accuracy result of the training
INFO 2017-08-07 19:01:56,485 - Number of layers: 1
INFO 2017-08-07 19:01:56,485 - Number of neurons: 256
INFO 2017-08-07 19:01:56,485 - Activation: selu
INFO 2017-08-07 19:01:56,485 - Optimizer: adam
INFO 2017-08-07 19:01:56,485 - Dropout: 0.2
INFO 2017-08-07 19:07:09,289 - Acc @ Training: 0%
INFO 2017-08-07 19:07:09,289 - Acc @ Testing: 46.25%
INFO 2017-08-07 19:07:09,289 - --------------------------------------------------------
INFO 2017-08-07 19:07:09,289 - Saving the accuracy result of the training
INFO 2017-08-07 19:07:09,289 - Number of layers: 3
INFO 2017-08-07 19:07:09,289 - Number of neurons: 256
INFO 2017-08-07 19:07:09,289 - Activation: sigmoid
INFO 2017-08-07 19:07:09,289 - Optimizer: adamax
INFO 2017-08-07 19:07:09,290 - Dropout: 0.1
INFO 2017-08-07 19:16:54,468 - Acc @ Training: 0%
INFO 2017-08-07 19:16:54,468 - Acc @ Testing: 52.6%
INFO 2017-08-07 19:16:54,468 - --------------------------------------------------------
INFO 2017-08-07 19:16:54,468 - Saving the accuracy result of the training
INFO 2017-08-07 19:16:54,468 - Number of layers: 1
INFO 2017-08-07 19:16:54,468 - Number of neurons: 512
INFO 2017-08-07 19:16:54,468 - Activation: selu
INFO 2017-08-07 19:16:54,468 - Optimizer: adamax
INFO 2017-08-07 19:16:54,468 - Dropout: 0.2
INFO 2017-08-07 19:32:27,070 - Acc @ Training: 0%
INFO 2017-08-07 19:32:27,070 - Acc @ Testing: 48.35%
INFO 2017-08-07 19:32:27,070 - --------------------------------------------------------
INFO 2017-08-07 19:32:27,070 - Saving the accuracy result of the training
INFO 2017-08-07 19:32:27,070 - Number of layers: 3
INFO 2017-08-07 19:32:27,070 - Number of neurons: 256
INFO 2017-08-07 19:32:27,070 - Activation: sigmoid
INFO 2017-08-07 19:32:27,070 - Optimizer: adamax
INFO 2017-08-07 19:32:27,070 - Dropout: 0.05
INFO 2017-08-07 19:39:36,873 - Acc @ Training: 0%
INFO 2017-08-07 19:39:36,873 - Acc @ Testing: 52.78%
INFO 2017-08-07 19:39:36,873 - --------------------------------------------------------
INFO 2017-08-07 19:39:36,873 - Saving the accuracy result of the training
INFO 2017-08-07 19:39:36,873 - Number of layers: 1
INFO 2017-08-07 19:39:36,873 - Number of neurons: 1024
INFO 2017-08-07 19:39:36,873 - Activation: sigmoid
INFO 2017-08-07 19:39:36,873 - Optimizer: adamax
INFO 2017-08-07 19:39:36,873 - Dropout: 0.1
INFO 2017-08-07 20:27:08,186 - Acc @ Training: 0%
INFO 2017-08-07 20:27:08,189 - Acc @ Testing: 53.51%
INFO 2017-08-07 20:27:08,189 - --------------------------------------------------------
INFO 2017-08-07 20:27:08,189 - Saving the accuracy result of the training
INFO 2017-08-07 20:27:08,190 - Number of layers: 3
INFO 2017-08-07 20:27:08,190 - Number of neurons: 1024
INFO 2017-08-07 20:27:08,190 - Activation: sigmoid
INFO 2017-08-07 20:27:08,190 - Optimizer: adamax
INFO 2017-08-07 20:27:08,190 - Dropout: 0.1





+562

INFO 2017-08-08 09:56:15,535 - Parents: [{'nb_layers': 3, 'nb_neurons': 1024, 'dropout': 0.1, 'activation': 'sigmoid', 'optimizer': 'adamax'}, {'nb_layers': 3, 'nb_neurons': 256, 'dropout': 0.15, 'activation': 'selu', 'optimizer': 'adamax'}, {'nb_layers': 4, 'nb_neurons': 1024, 'dropout': 0.05, 'activation': 'sigmoid', 'optimizer': 'adamax'}, {'nb_layers': 4, 'nb_neurons': 1024, 'dropout': 0.1, 'activation': 'sigmoid', 'optimizer': 'adamax'}, {'nb_layers': 4, 'nb_neurons': 512, 'dropout': 0.05, 'activation': 'sigmoid', 'optimizer': 'adamax'}, {'nb_layers': 3, 'nb_neurons': 1024, 'dropout': 0.15, 'activation': 'selu', 'optimizer': 'adamax'}]
INFO 2017-08-08 09:56:15,542 - *************************************************
INFO 2017-08-08 09:56:15,542 - ------ GEN 4 ------
INFO 2017-08-08 09:56:15,542 - Saving the accuracy result of the training
INFO 2017-08-08 09:56:15,542 - Number of layers: 3
INFO 2017-08-08 09:56:15,542 - Number of neurons: 1024
INFO 2017-08-08 09:56:15,542 - Activation: sigmoid
INFO 2017-08-08 09:56:15,542 - Optimizer: adam
INFO 2017-08-08 09:56:15,542 - Dropout: 0.1








+755

INFO 2017-08-08 15:37:58,099 - Parents: [{'nb_layers': 3, 'nb_neurons': 1024, 'optimizer': 'adamax', 'dropout': 0.15, 'activation': 'sigmoid'}, {'nb_layers': 3, 'nb_neurons': 1024, 'optimizer': 'adamax', 'dropout': 0.15, 'activation': 'sigmoid'}, {'nb_layers': 3, 'nb_neurons': 256, 'optimizer': 'adamax', 'dropout': 0.15, 'activation': 'selu'}, {'nb_layers': 3, 'nb_neurons': 1024, 'optimizer': 'adamax', 'dropout': 0.15, 'activation': 'sigmoid'}]
INFO 2017-08-08 15:37:58,099 - *************************************************
INFO 2017-08-08 15:37:58,101 - ------ GEN 5 ------
INFO 2017-08-08 15:37:58,101 - Saving the accuracy result of the training
INFO 2017-08-08 15:37:58,101 - Number of layers: 3
INFO 2017-08-08 15:37:58,101 - Number of neurons: 1024
INFO 2017-08-08 15:37:58,101 - Activation: sigmoid
INFO 2017-08-08 15:37:58,101 - Optimizer: adamax
INFO 2017-08-08 15:37:58,101 - Dropout: 0.15
INFO 2017-08-08 15:53:00,028 - Acc @ Training: 0%
INFO 2017-08-08 15:53:00,028 - Acc @ Testing: 54.6%
INFO 2017-08-08 15:53:00,028 - --------------------------------------------------------
INFO 2017-08-08 15:53:00,028 - Saving the accuracy result of the training
INFO 2017-08-08 15:53:00,028 - Number of layers: 3
INFO 2017-08-08 15:53:00,029 - Number of neurons: 1024
INFO 2017-08-08 15:53:00,029 - Activation: sigmoid
INFO 2017-08-08 15:53:00,029 - Optimizer: adamax
INFO 2017-08-08 15:53:00,029 - Dropout: 0.15







+940

INFO 2017-08-08 21:16:12,002 - Parents: [{'optimizer': 'adamax', 'nb_neurons': 1024, 'activation': 'sigmoid', 'dropout': 0.15, 'nb_layers': 3}, {'optimizer': 'adamax', 'nb_neurons': 1024, 'activation': 'sigmoid', 'dropout': 0.15, 'nb_layers': 3}, {'optimizer': 'adamax', 'nb_neurons': 1024, 'activation': 'sigmoid', 'dropout': 0.15, 'nb_layers': 3}, {'optimizer': 'adamax', 'nb_neurons': 1024, 'activation': 'sigmoid', 'dropout': 0.15, 'nb_layers': 3}]
INFO 2017-08-08 21:16:12,002 - *************************************************
INFO 2017-08-08 21:16:12,003 - ------ GEN 6 ------
INFO 2017-08-08 21:16:12,003 - Saving the accuracy result of the training
INFO 2017-08-08 21:16:12,003 - Number of layers: 3
INFO 2017-08-08 21:16:12,003 - Number of neurons: 1024
INFO 2017-08-08 21:16:12,003 - Activation: sigmoid
INFO 2017-08-08 21:16:12,003 - Optimizer: adamax
INFO 2017-08-08 21:16:12,003 - Dropout: 0.15
INFO 2017-08-08 21:33:26,182 - Acc @ Training: 0%
INFO 2017-08-08 21:33:26,182 - Acc @ Testing: 54.92%
INFO 2017-08-08 21:33:26,182 - --------------------------------------------------------
INFO 2017-08-08 21:33:26,182 - Saving the accuracy result of the training
INFO 2017-08-08 21:33:26,182 - Number of layers: 3
INFO 2017-08-08 21:33:26,182 - Number of neurons: 1024
INFO 2017-08-08 21:33:26,182 - Activation: sigmoid
INFO 2017-08-08 21:33:26,182 - Optimizer: nadam
INFO 2017-08-08 21:33:26,182 - Dropout: 0.15
INFO 2017-08-08 21:56:06,923 - Acc @ Training: 0%
INFO 2017-08-08 21:56:06,923 - Acc @ Testing: 38.87%
INFO 2017-08-08 21:56:06,923 - --------------------------------------------------------
INFO 2017-08-08 21:56:06,923 - Saving the accuracy result of the training
INFO 2017-08-08 21:56:06,923 - Number of layers: 3
INFO 2017-08-08 21:56:06,923 - Number of neurons: 1024
INFO 2017-08-08 21:56:06,923 - Activation: sigmoid
INFO 2017-08-08 21:56:06,924 - Optimizer: adamax
INFO 2017-08-08 21:56:06,924 - Dropout: 0.15





+932

INFO 2017-08-09 09:30:09,142 - Cifar10 data retrieved & Processed
INFO 2017-08-09 09:30:09,142 - Starting Proof of Concept
INFO 2017-08-09 09:30:09,142 - Number of layers: [1, 2, 3, 4, 5]
INFO 2017-08-09 09:30:09,142 - Number of neurons: [32, 64, 128, 256, 512, 768, 1024]
INFO 2017-08-09 09:30:09,142 - Activation: ['tanh', 'sigmoid', 'relu', 'elu', 'selu', 'linear']
INFO 2017-08-09 09:30:09,142 - Optimizer: ['sgd', 'adamax', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'nadam']
INFO 2017-08-09 09:30:09,142 - Dropout: [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4]
INFO 2017-08-09 09:30:09,142 - /////////////////////////////////////////////////////
INFO 2017-08-09 09:30:09,143 - Parents: [{'optimizer': 'adamax', 'nb_neurons': 1024, 'dropout': 0.15, 'activation': 'sigmoid', 'nb_layers': 3}, {'optimizer': 'adamax', 'nb_neurons': 1042, 'dropout': 0.15, 'activation': 'sigmoid', 'nb_layers': 3}, {'optimizer': 'adamax', 'nb_neurons': 1024, 'dropout': 0.15, 'activation': 'sigmoid', 'nb_layers': 3}, {'optimizer': 'adamax', 'nb_neurons': 1024, 'dropout': 0.15, 'activation': 'sigmoid', 'nb_layers': 3}]
INFO 2017-08-09 09:30:09,143 - *************************************************
INFO 2017-08-09 09:30:09,144 - ------ GEN 6 ------
INFO 2017-08-09 09:30:09,144 - Saving the accuracy result of the training
INFO 2017-08-09 09:30:09,144 - Number of layers: 3
INFO 2017-08-09 09:30:09,144 - Number of neurons: 1024
INFO 2017-08-09 09:30:09,144 - Activation: sigmoid
INFO 2017-08-09 09:30:09,144 - Optimizer: adamax
INFO 2017-08-09 09:30:09,144 - Dropout: 0.15
INFO 2017-08-09 09:46:34,672 - Acc @ Training: 0%
INFO 2017-08-09 09:46:34,678 - Acc @ Testing: 54.47%
INFO 2017-08-09 09:46:34,678 - --------------------------------------------------------
INFO 2017-08-09 09:46:34,678 - Saving the accuracy result of the training
INFO 2017-08-09 09:46:34,678 - Number of layers: 3
INFO 2017-08-09 09:46:34,678 - Number of neurons: 1024
INFO 2017-08-09 09:46:34,678 - Activation: sigmoid
INFO 2017-08-09 09:46:34,678 - Optimizer: adamax
INFO 2017-08-09 09:46:34,678 - Dropout: 0.15
INFO 2017-08-09 10:00:45,379 - Acc @ Training: 0%
INFO 2017-08-09 10:00:45,379 - Acc @ Testing: 54.5%
INFO 2017-08-09 10:00:45,379 - --------------------------------------------------------
INFO 2017-08-09 10:00:45,379 - Saving the accuracy result of the training
INFO 2017-08-09 10:00:45,380 - Number of layers: 3
INFO 2017-08-09 10:00:45,380 - Number of neurons: 1024
INFO 2017-08-09 10:00:45,380 - Activation: sigmoid
INFO 2017-08-09 10:00:45,380 - Optimizer: adamax
INFO 2017-08-09 10:00:45,380 - Dropout: 0.15
INFO 2017-08-09 10:13:38,802 - Acc @ Training: 0%
INFO 2017-08-09 10:13:38,802 - Acc @ Testing: 52.45%
INFO 2017-08-09 10:13:38,802 - --------------------------------------------------------
INFO 2017-08-09 10:13:38,802 - Saving the accuracy result of the training
INFO 2017-08-09 10:13:38,802 - Number of layers: 3
INFO 2017-08-09 10:13:38,802 - Number of neurons: 1024
INFO 2017-08-09 10:13:38,802 - Activation: relu
INFO 2017-08-09 10:13:38,802 - Optimizer: adamax
INFO 2017-08-09 10:13:38,802 - Dropout: 0.15
INFO 2017-08-09 10:24:35,489 - Acc @ Training: 0%
INFO 2017-08-09 10:24:35,489 - Acc @ Testing: 52.89%
INFO 2017-08-09 10:24:35,489 - --------------------------------------------------------
INFO 2017-08-09 10:24:35,489 - Saving the accuracy result of the training
INFO 2017-08-09 10:24:35,489 - Number of layers: 3
INFO 2017-08-09 10:24:35,489 - Number of neurons: 1024
INFO 2017-08-09 10:24:35,489 - Activation: relu
INFO 2017-08-09 10:24:35,489 - Optimizer: adamax
INFO 2017-08-09 10:24:35,489 - Dropout: 0.15
INFO 2017-08-09 10:40:40,220 - Acc @ Training: 0%
INFO 2017-08-09 10:40:40,221 - Acc @ Testing: 53.36%
INFO 2017-08-09 10:40:40,221 - --------------------------------------------------------
INFO 2017-08-09 10:40:40,221 - Saving the accuracy result of the training
INFO 2017-08-09 10:40:40,221 - Number of layers: 3
INFO 2017-08-09 10:40:40,221 - Number of neurons: 1042
INFO 2017-08-09 10:40:40,221 - Activation: sigmoid
INFO 2017-08-09 10:40:40,221 - Optimizer: adamax
INFO 2017-08-09 10:40:40,221 - Dropout: 0.15
INFO 2017-08-09 11:03:12,149 - Acc @ Training: 0%
INFO 2017-08-09 11:03:12,150 - Acc @ Testing: 55.55%
INFO 2017-08-09 11:03:12,150 - --------------------------------------------------------
INFO 2017-08-09 11:03:12,150 - Saving the accuracy result of the training
INFO 2017-08-09 11:03:12,150 - Number of layers: 3
INFO 2017-08-09 11:03:12,150 - Number of neurons: 1024
INFO 2017-08-09 11:03:12,150 - Activation: sigmoid
INFO 2017-08-09 11:03:12,150 - Optimizer: adamax
INFO 2017-08-09 11:03:12,150 - Dropout: 0.15
INFO 2017-08-09 11:18:24,382 - Acc @ Training: 0%
INFO 2017-08-09 11:18:24,382 - Acc @ Testing: 55.11%
INFO 2017-08-09 11:18:24,382 - --------------------------------------------------------
INFO 2017-08-09 11:18:24,382 - Saving the accuracy result of the training
INFO 2017-08-09 11:18:24,382 - Number of layers: 3
INFO 2017-08-09 11:18:24,382 - Number of neurons: 1024
INFO 2017-08-09 11:18:24,383 - Activation: relu
INFO 2017-08-09 11:18:24,383 - Optimizer: adamax
INFO 2017-08-09 11:18:24,383 - Dropout: 0.15
INFO 2017-08-09 11:28:48,252 - Acc @ Training: 0%
INFO 2017-08-09 11:28:48,252 - Acc @ Testing: 51.33%
INFO 2017-08-09 11:28:48,252 - --------------------------------------------------------
INFO 2017-08-09 11:28:48,252 - Saving the accuracy result of the training
INFO 2017-08-09 11:28:48,252 - Number of layers: 3
INFO 2017-08-09 11:28:48,252 - Number of neurons: 1024
INFO 2017-08-09 11:28:48,252 - Activation: sigmoid
INFO 2017-08-09 11:28:48,252 - Optimizer: adamax
INFO 2017-08-09 11:28:48,252 - Dropout: 0.15
INFO 2017-08-09 11:43:14,763 - Acc @ Training: 0%
INFO 2017-08-09 11:43:14,763 - Acc @ Testing: 53.08%
INFO 2017-08-09 11:43:14,763 - --------------------------------------------------------
INFO 2017-08-09 11:43:14,763 - Saving the accuracy result of the training
INFO 2017-08-09 11:43:14,763 - Number of layers: 3
INFO 2017-08-09 11:43:14,763 - Number of neurons: 1024
INFO 2017-08-09 11:43:14,763 - Activation: sigmoid
INFO 2017-08-09 11:43:14,763 - Optimizer: adamax
INFO 2017-08-09 11:43:14,763 - Dropout: 0.15
INFO 2017-08-09 12:00:42,065 - Acc @ Training: 0%
INFO 2017-08-09 12:00:42,066 - Acc @ Testing: 55.03%
INFO 2017-08-09 12:00:42,066 - --------------------------------------------------------
INFO 2017-08-09 12:00:42,066 - Saving the accuracy result of the training
INFO 2017-08-09 12:00:42,066 - Number of layers: 3
INFO 2017-08-09 12:00:42,066 - Number of neurons: 1024
INFO 2017-08-09 12:00:42,066 - Activation: relu
INFO 2017-08-09 12:00:42,066 - Optimizer: adamax
INFO 2017-08-09 12:00:42,066 - Dropout: 0.15
INFO 2017-08-09 12:16:32,818 - Acc @ Training: 0%
INFO 2017-08-09 12:16:32,818 - Acc @ Testing: 52.95%
INFO 2017-08-09 12:16:32,818 - --------------------------------------------------------
INFO 2017-08-09 12:16:32,818 - Saving the accuracy result of the training
INFO 2017-08-09 12:16:32,818 - Number of layers: 3
INFO 2017-08-09 12:16:32,818 - Number of neurons: 1042
INFO 2017-08-09 12:16:32,818 - Activation: sigmoid
INFO 2017-08-09 12:16:32,818 - Optimizer: adamax
INFO 2017-08-09 12:16:32,818 - Dropout: 0.15
INFO 2017-08-09 12:30:20,784 - Acc @ Training: 0%
INFO 2017-08-09 12:30:20,784 - Acc @ Testing: 53.85%
INFO 2017-08-09 12:30:20,784 - --------------------------------------------------------
INFO 2017-08-09 12:30:20,784 - Saving the accuracy result of the training
INFO 2017-08-09 12:30:20,784 - Number of layers: 3
INFO 2017-08-09 12:30:20,784 - Number of neurons: 1024
INFO 2017-08-09 12:30:20,784 - Activation: relu
INFO 2017-08-09 12:30:20,784 - Optimizer: adamax
INFO 2017-08-09 12:30:20,784 - Dropout: 0.15
INFO 2017-08-09 12:46:36,956 - Acc @ Training: 0%
INFO 2017-08-09 12:46:36,956 - Acc @ Testing: 52.47%
INFO 2017-08-09 12:46:36,956 - --------------------------------------------------------
INFO 2017-08-09 12:46:36,956 - Saving the accuracy result of the training
INFO 2017-08-09 12:46:36,956 - Number of layers: 3
INFO 2017-08-09 12:46:36,956 - Number of neurons: 1042
INFO 2017-08-09 12:46:36,956 - Activation: sigmoid
INFO 2017-08-09 12:46:36,956 - Optimizer: adamax
INFO 2017-08-09 12:46:36,956 - Dropout: 0.15
INFO 2017-08-09 12:59:33,897 - Acc @ Training: 0%
INFO 2017-08-09 12:59:33,898 - Acc @ Testing: 54.18%
INFO 2017-08-09 12:59:33,898 - --------------------------------------------------------
INFO 2017-08-09 12:59:33,898 - Saving the accuracy result of the training
INFO 2017-08-09 12:59:33,898 - Number of layers: 3
INFO 2017-08-09 12:59:33,898 - Number of neurons: 1024
INFO 2017-08-09 12:59:33,898 - Activation: relu
INFO 2017-08-09 12:59:33,898 - Optimizer: adamax
INFO 2017-08-09 12:59:33,898 - Dropout: 0.15
INFO 2017-08-09 13:10:50,998 - Acc @ Training: 0%
INFO 2017-08-09 13:10:50,998 - Acc @ Testing: 52.2%
INFO 2017-08-09 13:10:50,999 - --------------------------------------------------------
INFO 2017-08-09 13:10:50,999 - Saving the accuracy result of the training
INFO 2017-08-09 13:10:50,999 - Number of layers: 3
INFO 2017-08-09 13:10:50,999 - Number of neurons: 1024
INFO 2017-08-09 13:10:50,999 - Activation: sigmoid
INFO 2017-08-09 13:10:50,999 - Optimizer: adamax
INFO 2017-08-09 13:10:50,999 - Dropout: 0.15
INFO 2017-08-09 13:29:47,895 - Acc @ Training: 0%
INFO 2017-08-09 13:29:47,895 - Acc @ Testing: 54.84%
INFO 2017-08-09 13:29:47,895 - --------------------------------------------------------
INFO 2017-08-09 13:29:47,896 - Saving the accuracy result of the training
INFO 2017-08-09 13:29:47,896 - Number of layers: 3
INFO 2017-08-09 13:29:47,896 - Number of neurons: 1042
INFO 2017-08-09 13:29:47,896 - Activation: sigmoid
INFO 2017-08-09 13:29:47,896 - Optimizer: adamax
INFO 2017-08-09 13:29:47,896 - Dropout: 0.15
INFO 2017-08-09 13:40:54,096 - Acc @ Training: 0%
INFO 2017-08-09 13:40:54,096 - Acc @ Testing: 53.13%
INFO 2017-08-09 13:40:54,096 - --------------------------------------------------------
INFO 2017-08-09 13:40:54,096 - Saving the accuracy result of the training
INFO 2017-08-09 13:40:54,096 - Number of layers: 3
INFO 2017-08-09 13:40:54,096 - Number of neurons: 1024
INFO 2017-08-09 13:40:54,096 - Activation: sigmoid
INFO 2017-08-09 13:40:54,097 - Optimizer: adamax
INFO 2017-08-09 13:40:54,097 - Dropout: 0.15
INFO 2017-08-09 14:00:40,953 - Acc @ Training: 0%
INFO 2017-08-09 14:00:40,954 - Acc @ Testing: 55.35%
INFO 2017-08-09 14:00:40,954 - --------------------------------------------------------
INFO 2017-08-09 14:00:40,954 - Saving the accuracy result of the training
INFO 2017-08-09 14:00:40,954 - Number of layers: 3
INFO 2017-08-09 14:00:40,954 - Number of neurons: 1024
INFO 2017-08-09 14:00:40,954 - Activation: relu
INFO 2017-08-09 14:00:40,954 - Optimizer: adamax
INFO 2017-08-09 14:00:40,954 - Dropout: 0.15